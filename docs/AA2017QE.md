# Unbiased instrumental variables estimation under known first-stage sign (Andrews and Armstrong, 2017)

## Settings

$$\text{Reduced-form:} \quad Y = Z \pi \beta + U \\
\text{First-stage:} \quad X = Z \pi + V$$

- $Y$ and $X$ are $N \times 1$ vectors and $Z$ is $N \times k$ matrix
- When there are other covariates, say, $W$, we can consider $Y$, $X$, $Z$ as regression residuals on $W$
- $(U, V) \sim N(\bm{0}, \Sigma)$, where $\Sigma$ is assumed to be known
- Denoting the reduced-form and 1st-stage regression coefficients as $\xi_1$ and $\xi_2$, respectively, observe
    $$\begin{pmatrix}
    \xi_1 \\
    \xi_2
    \end{pmatrix}
    \sim N \left(
        \begin{pmatrix}
            \pi \beta \\
            \pi
        \end{pmatrix},
        \begin{pmatrix}
            \Sigma_{11}, \Sigma_{12} \\
            \Sigma_{21}, \Sigma_{22}
        \end{pmatrix}
    \right)$$

- An important assumption of this study is that the sign of the 1st-stage coefficient is known, or the parameter space for $(\pi, \beta)$ is
    $$\Theta = \{(\pi, \beta): \pi \in \Pi \subseteq (0, \infty)^k, \beta \in \mathcal{B} \}$$

## Unbiased estimation with a single instrument

- Let $\sigma_{ij} = \Sigma_{ij}$
- In this case, the estimand is
    $$\beta = \frac{\pi \beta}{\pi} = \frac{E[\xi_1]}{E[\xi_2]}$$

- A natural sample analog is the conventional IV estimator: $\hat{\beta}_{2SLS} = \frac{\xi_1}{\xi_2}$
- However, this estimator is biased as
    $$E \left[\hat{\beta}_{2SLS} \right] = E \left[\frac{\xi_1}{\xi_2} \right] \neq \frac{E[\xi_1]}{E[\xi_2]} = \beta$$

- If there is an unbiased estimator of $\frac{1}{\pi}$ that is indepedent of $\xi_1$, the unbiased estimator can be constructed
- Orthogonalize $\xi_1$ with respect to $\xi_2$
    $$\delta(\xi, \Sigma) = \xi_1 - \frac{\sigma_{12}}{\sigma_{22}^2} \xi_2 \quad \Rightarrow \quad \delta(\xi, \Sigma) \text{ is independent of $\xi_2$}$$
    ($\because$ $\delta$ is normal and $Cov(\delta, \xi_2) = 0$)
- Voinov and Nikulin (1993) shows that unbiased estimation of $\frac{1}{\pi}$ is possible if we assume its sign is known

### Lemma 2.1

Define
$$\hat{\tau} (\xi_2, \sigma_{22}) = \frac{1}{\sigma_{22}} \frac{1 - \Phi\left(\frac{\xi_2}{\sigma_{22}} \right)}{\phi\left(\frac{\xi_2}{\sigma_{22}} \right)}$$

For all $\tau > 0$, $E[\hat{\tau}(\xi_2, \sigma_{22})] = \frac{1}{\pi}$. (Note that $\hat{\tau}(\xi_2, \sigma_{22}) > 0$ for any $(\xi_2, \sigma_{22}) \in \mathbb{R}^2$, which is why the sign restriction is necessary)

#### Proof of Lemma 2.1

By the definition of the standard normal density and the integral by parts,
$$E[\hat{\tau}(\xi_2, \sigma_{22})] = - \frac{\sigma_{22}\exp\left(-\frac{\pi^2}{2 \sigma_{22}^2}\right)}{\pi} \left[\Phi\left(-\frac{\xi_2}{\sigma_{22}}\right) \exp\left(\frac{\pi}{\sigma_{22}^2} \xi_2 \right)\right]_{u=-\infty}^{\infty} + \frac{1}{\pi}$$

From  L'Hospital's rule, for some constatnt $C$,
$$\lim_{\xi_2 \rightarrow \infty} \Phi\left(-\frac{\xi_2}{\sigma_{22}}\right) \exp\left(\frac{\pi}{\sigma_{22}^2} \xi_2 \right) = C \lim_{\xi_2 \rightarrow \infty} \exp\left(- \frac{(\xi_2 - \pi)^2 - \pi^2}{2\sigma_{22}^2} \right) = 0$$

Hence, the desired result is obtained

### Theorem 2.1

Define
$$\hat{\beta}_U (\xi_2, \Sigma) = \hat{\tau}(\xi_2, \sigma_{22}) \hat{\delta}(\xi, \Sigma) + \frac{\sigma_{12}}{\sigma_{22}^2}$$

The estimator $\hat{\beta}_U (\xi_2, \Sigma)$ is unbiased for $\beta$ provided $\pi > 0$

#### Proof of Theorem 2.1

Given that $\hat{\tau}(\xi_2, \sigma_{22})$ is independent of $\hat{\delta}(\xi, \Sigma)$ and unbiased for $\frac{1}{\pi}$,
$$E \left[\hat{\beta}_U (\xi_2, \Sigma) \right] = \frac{1}{\pi} \left[\pi \beta - \frac{\sigma_{12}}{\sigma_{22}^2} \pi \right] + \frac{\sigma_{12}}{\sigma_{22}^2} = \beta$$

#### Note

The convetional IV estimator is written

$$\hat{\beta}_{2SLS} = \frac{1}{\xi_2} \left(\xi_1 - \frac{\sigma_{12}}{\sigma_{22}^2} \xi_2 \right) + \frac{\sigma_{12}}{\sigma_{22}^2}$$

Thus, $\hat{\beta}_U$ differs from the conventional IV estimator $\hat{\beta}_{2SLS}$ only in that it replaces the plug-in estimate $\frac{1}{\xi_2}$ for $\tau$ by the unbiased estimate $\frac{1}{\pi}$

## Behavior of $\hat{\beta}_U$ when $\pi$ is large

- $\hat{\tau} (\xi_2, \sigma_22)$ converges to $\frac{1}{\xi_2}$ as $\frac{\xi_2}{\sigma_{22}} \rightarrow \infty$ <br>
    $\because$ By L'Hospital's rule
    $$\lim_{\frac{\xi_2}{\sigma_{22}} \rightarrow \infty} \hat{\tau} (\xi_2, \sigma_{22}) = \frac{1}{\xi_2} \lim_{x \rightarrow \infty} \frac{1 - \Phi(x)}{\phi(x) / x} = \frac{1}{\xi_2} \lim_{x \rightarrow \infty} \frac{\phi(x)}{\phi(x) \left(1 + \frac{1}{x^2}\right)} = \frac{1}{\xi_2}$$

- Furthermore, Small (2010) shows that, for $\xi_2 > 0$,
    $$\sigma_{22} \left|\hat{\tau} \left(\xi_2, \sigma_{22} \right) - \frac{1}{\xi_2} \right| \leq \left| \frac{\sigma_{22}^3}{\xi_2^3} \right|$$

### Theorem 2.3

As $\pi \rightarrow \infty$, holding $\beta$ and $\Sigma$ fixed,
$$\pi \left(\hat{\beta}_U - \hat{\beta}_{2SLS} \right) \rightarrow_p  0$$

## Unbiased estimation with multiple instruments

We would like to find an estimator that is
- unbiased for fixed $\pi$
- asymptotically equivalent to 2SLS as $\|\pi\| \rightarrow \infty$

Let
$$\xi (i) =
\begin{pmatrix}
    \xi_{1i} \\
    \xi_{2i}
\end{pmatrix}; \qquad
\Sigma(i) =
\begin{pmatrix}
    \Sigma_{11, i} & \Sigma_{12, i} \\
    \Sigma_{21, i} & \Sigma_{22, i}
\end{pmatrix}$$

- Then
    $$\hat{\beta}_w(\xi, \Sigma; w) = \sum_{i=1}^{k} w_i \hat{\beta}_U \left(\xi (i), \Sigma (i) \right)$$
    is unbiased for $\beta$ as long as $w \in \Delta^{k-1}$ is non-random
- More generally, if $w$ is indepedenet of $\hat{\beta}_U$, $\hat{\beta}_w(\xi, \Sigma; w)$ is unbiased
- Hence, construct $w$ and $\hat{beta}_U$ from indepedent random variables
    1. Draw $\zeta \sim N(0, \Sigma)$ (independent of $\xi$)
    2. Define
        $$\xi^{(a)} = \xi + \zeta \sim N(0, 2 \Sigma) \\
        \xi^{(b)} = \xi - \zeta \sim N(0, 2 \Sigma)$$
        Note that $Cov \left(\xi^{(a)}, \xi^{(b)}\right) = 0$, and hence $\xi^{(a)}$ and $\xi^{(b)}$ are independent
    3. Construct the unbiased estimate based on $\xi^{(a)}$: $\hat{\beta}_U \left(\xi^{(a)} (i), 2\Sigma (i) \right)$
    4. Construct the weight based on $\xi^{(b)}$: $\hat{w}\left(\xi^{(b)}\right)$
    5. Define Rao-Blackwellized estimator as
        $$\hat{\beta}_{RB} \left(\xi, \Sigma, \hat{w} \right) = E \left[\hat{\beta}_w \left(\xi^{(a)}, 2\Sigma, \hat{w}\left(\xi^{(b)} \right) \right) \Big| \xi \right] \\
        = \sum_{i=1}^{k} E\left[\hat{w}_i \left(\xi^{(b)} \right) \Big| \xi \right] E \left[\hat{\beta}_U \left(\xi^{(a)} (i), \Sigma (i) \right) \Big| \xi \right]$$
- The unbiasedness is immediately follows fromthe above equation and the law of iterated equations:
    $$E\left[\hat{\beta}_{RB} \left(\xi, \Sigma, \hat{w} \right)\right] = \sum_{i=1}^{k} E\left[\hat{w}_i \left(\xi^{(b)} \right) \right] E \left[\hat{\beta}_U \left(\xi^{(a)} (i), \Sigma (i) \right) \right] = \beta$$

### Equivalence with 2SLS under strong-IV asymptotics

- Note that
    $$\hat{\beta}_{2SLS} = \frac{\xi_2' W \xi_1}{\xi_2' W \xi_2} = \sum_{i=1}^{k} \left(\frac{\xi_2' W e_i e_i' \xi_2}{\xi_2' W \xi_2} \right)\frac{\xi_{1i}}{\xi_{2i}}$$
    where $W = Z'Z$ and $e_i$ is the $i$th standard base vector
- Hence, natural chice of the weight for the unbiased estimator is
    $$\hat{w}^{*}\left(\xi^{(b)}\right) = \frac{\xi^{(b)'}_2 W e_i e_i' \xi^{(b)}_2}{\xi_2^{(b)'} W \xi_2^{(b)}}$$

### Theorem 3.1

Define
$$\hat{\beta}_{RB}^{*} = \hat{\beta}_{RB}^{*} \left(\xi, \Sigma, \hat{w} \right) = E \left[\hat{\beta}_w \left(\xi^{(a)}, 2\Sigma, \hat{w}^{*}\left(\xi^{(b)} \right) \right) \Big| \xi \right]$$
Let $\|\pi\| \rightarrow \infty$ with $\|\pi\| / \min_i \pi_i = \mathcal{O}(1)$. Then
$$\|\pi\| \left(\hat{\beta}_{RB}^{*} - \hat{\beta}_{2SLS} \right) \rightarrow_p 0$$

## Robust unbiased estimation

- Unbiased estimation of $\beta$ requires the knowledge of the sign of the 1st-stage coefficients $\pi$; $\pi_i > 0$ for all $i = 1, \dots, k$
- We relax this assumption by using linear transformation of the instruments, instead of $Z$ itself
  - Let $M$ be a $k \times k$ matrix all of whose elements are strictly positive: $M = \left(m_{ij}\right)$
  - Use $ZM^{-1}$ as instruments
  - Then, the 1st-stage coefficients are $M\pi$
    $$(M\pi)_i = \sum_{j=1}^k m_{ij} \pi_j$$
- $M\pi$ is more likely to satisfy the sign assumption, as $\pi > 0 \Rightarrow M \pi > 0$